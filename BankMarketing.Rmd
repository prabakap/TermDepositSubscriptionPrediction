---
title: "Term Deposit Subscription Prediction"
author: "Pradeepkumar Prabakaran"
subtitle: "Balancing Class Imbalance problem"
output:
  html_document:
    keep_md: TRUE
    code_folding: hide
    df_print: kable
    highlight: monochrome
    theme: readable
    toc: yes
    toc_depth: 6
    toc_float: no
---

##Summary

Our [dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Our objective here is to analyze and find the important feature which leads to the **product (bank term deposit) subscription** so our bank can optimize the future marketing campaigns.

In short, our objective is **to predict whether the customer will subscribe to our marketing campaign** - [Classification Problem](https://en.wikipedia.org/wiki/Statistical_classification). Here we also face [Class Imbalance problem](http://www.chioka.in/class-imbalance-problem/) - Our dependent variable is *not equally distributed*, the number of customer who subscribed our campaign are very less when compared to the other, this often lead to the [Accuracy Paradox Problem](https://en.wikipedia.org/wiki/Accuracy_paradox), if our model blindly predict all of our customer will not subscribe we will have our *Accuracy Score closely to 88%*. Below let's see how can we overcome such problem and build a model which can effectively predict the future or unseen data.


```{r loading-packages, message=FALSE,warning=FALSE}
library(tidyverse)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(caret)
library(randomForest)
library(DMwR)
library(corrplot)
library(pROC)
library(ROCR)
library(forcats)
library(knitr)
library(kableExtra)
```

##Exploratory Data Analysis

###Data

You can download this [Dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) UCI Machine Learning Repository. Here I'm going to follow the [Tidyverse](https://www.tidyverse.org/) way for the analysis. 

```{r loading-data,message=FALSE,warning=FALSE}
setwd('C:/Users/Pradeeepkumar/Documents/Projects/TermDepositSubscriptionPrediction')
market_data<-read_delim('bank-full.csv', delim=';')
glimpse(market_data)
```

After you had a glimpse at our dataset you can see that all our character variables must be converted into factor. Apart from that day is also converted into factor.

```{r char-factor-convert,message=FALSE,warning=FALSE}
market_data <- market_data %>% mutate_if(is.character,as.factor)
market_data$day <- as.factor(market_data$day)
glimpse(market_data)
```

Luckily, we don't have NA's in our Data Set. Check total number of NA's for all our variables.

```{r na-check}
colSums(is.na(market_data))
```
Using skim function, we can easily skim through the distribution of our numeric variable.

```{r skim,results="hide"}
market_data %>% select_if(is.numeric) %>% skimr::skim()
```
![](skim_output.PNG)

###Variables correlation plot

Calculate the Correlation score of all numeric independent variables. We can see **pdays and previous** variables have the high correlation score of **0.44**. To avoid [multi-collinearity problem](http://www.statisticssolutions.com/multicollinearity/) it's advisable to remove one of the variables while building our model, so *pdays* is removed from our dataset.

```{r correlation-plot}
market_data %>% select_if(is.numeric) %>% cor() %>% corrplot(method = 'circle')
market_data<- subset(market_data, select = -c(pdays))
```

###EDA Plots

```{r Duration-Outcome-Boxplot,message=FALSE}
ggplot() + 
  geom_boxplot(data=market_data, aes(x=y, y=duration, fill=y))+
  labs(title = "Duration Vs Outcome Boxplot")+
  labs(x = "Client Response", y = "Call Duration (sec)",caption = "Data source: UCI")+theme_classic()
```

```{r Age-Outcome-Boxplot}
ggplot() + 
  geom_boxplot(data=market_data, aes(x=y, y=age, fill=y))+
  labs(title = "Age Vs Outcome Boxplot")+
  labs(x = "Client Response", y = "Age",caption = "Data source: UCI")+theme_classic()
```

```{r Education-Balance-Barplot}
ggplot() + 
  geom_bar(data=market_data, aes(x=education  , y=balance, fill=marital),stat = "identity", position = "dodge")+
  labs(title = "Education and Balance Bar Plot")+
  labs(x = "Education ", y = "Balance",caption = "Data source: UCI")+theme_classic()
```

Here new function [fct_relevel](https://www.rdocumentation.org/packages/forcats/versions/0.2.0/topics/fct_relevel) function is used to relevel our categorical variables for easy understanding of our plot.

```{r prev-MarketingCampaign-present}
ggplot(market_data, aes(x = fct_relevel(poutcome,'failure','success','other','unknown'), fill = y)) + geom_bar(stat="count",position = "dodge")+
  labs(title = "Previous Marketing campaign Count Vs Present")+
  labs(x = "Previous Marketing Campaign Outcome", y='Count',caption = "Data source: UCI")+theme_classic()+  theme(
    text = element_text(
      color = "gray25")
  )
```


##Model Preparation

Here to show the effects of **Class Imbalance Problem** we are going to run our Decision Tree model twice - Before solving Class Imbalance and After

###Before SMOTE 

'no' and 'yes' from our dependent variables are recoded as 0 and 1. The distribution of Customer Subscribed is **11.7%** and Customer not Subscribed is **88.3%**

```{r y-recode}
levels(market_data$y) <- c("0", "1")
round(prop.table(table(market_data$y))*100,2)
```

Set seed and split into train and test set - train (70% of data) and test (30% of data)

```{r train-test}
set.seed(1234)
splitIndex <- createDataPartition(market_data$y, p = .70,
                                  list = FALSE, times = 1)
trainSplit <- market_data[ splitIndex,]
testSplit <- market_data[-splitIndex,]
```

Proportion of response variable(y) in Training dataset before SMOTE

```{r train-beforeSMOTE}
round(prop.table(table(trainSplit$y)),2)
```

#### Decision Tree Classifier Before SMOTE

Decision Tree chooses its split based on the Highest Information gain received from each split which in turn results in more homogeneous leaf nodes. From this tree we can find **duration and poutcome** are important variables

```{r Decision-Classifier-BeforeSMOTE}
fit_1 <- rpart(y ~ .,
             data=trainSplit,
             method="class")

fancyRpartPlot(fit_1)

Prediction_1 <- predict(fit_1, testSplit, type = "class")
```

Confusion Matrix Before SMOTE - Here we can see that our Recall score is very less when compared to the precision score, our main objective is to boost our Recall score because we are concerned about predicting the customer who will subscribe and our model should also give better F1 score.

```{r Confusion-Matrix-DT-BeforeSMOTE}
DT_Before_CM<- confusionMatrix(Prediction_1, testSplit$y,positive ='1',mode = "prec_recall")
DT_Before_CM
```

###After SMOTE

[SMOTE](https://arxiv.org/pdf/1106.1813.pdf) is used to solve the Class Imbalance problem. Basic objective is to  come up with the proportion of 50-50 for negative and positive cases. That can be achieved by up sampling the positive class and down sampling our negative class, a hybrid approach. Here we set perc.over = 100 to double (Up Sample) the quantity of positive cases, and set perc.under=200 (Down sample) the negative cases.

```{r train-test-AfterSMOTE}
trainSplit<-as.data.frame(trainSplit)
trainSplit_smote <- SMOTE(y ~ ., trainSplit, perc.over = 100, perc.under=200)
```

Proportion of response variable in Train dataset after SMOTE. Our 50-50 dream is achieved.

```{r train-AfterSMOTE}
round(prop.table(table(trainSplit_smote$y)),2)
```

####Decision Tree Classifier After SMOTE

From this tree we can find **duration and previous** are important variables

```{r Decision-Classifier-AfterSMOTE}
fit_2 <- rpart(y ~ .,
             data=trainSplit_smote,
             method="class")
#, control = rpart.control(maxdepth=3)

fancyRpartPlot(fit_2)

Prediction_2 <- predict(fit_2, testSplit, type = "class")
```

Confusion Matrix After SMOTE - Here we can see the increase in our Recall score and F-1 score. This shows that our objectives are met.Run advanced models to see if you can further increase the F-1 score.

```{r Confusion-Matrix-DT-AfterSMOTE}
DT_After_CM<-confusionMatrix(Prediction_2, testSplit$y,positive = '1', mode = "prec_recall")
DT_After_CM$byClass
```

####Random Forest Classifier After SMOTE

```{r RF-AfterSMOTE}
randomForest_fit <- randomForest(y ~ .,
             data=trainSplit_smote,importance=TRUE,
             method="class")

Prediction_3 <- predict(randomForest_fit, testSplit, type = "class")
```

Again, we can see an increase in our Recall and F1 Score. A significant increase when compared to Before SMOTE part. This clearly shows how solving Class-Imbalance can significantly improve our model performance.

```{r CM-RF-AfterSMOTE}
RF_After_CM<-confusionMatrix(Prediction_3, testSplit$y,positive = '1', mode = "prec_recall")
RF_After_CM
```

By considering varImpPlot we can get an idea about important variables detected by our RandomForest model. From our RF model and DT model we can confirm that **duration and previous** are our important variables.

```{r RF-varImpPlot}
varImpPlot(randomForest_fit)
```

##Model Comparision

The below table clearly shows the performance of our models with respect to F1, Recall and Accuracy Scores

```{r}

models_table <- data.frame(models=c("Decision Tree Before SMOTE","Decision Tree After SMOTE","Random Forest After SMOTE"),
                        F1_Score=c(DT_Before_CM$byClass['F1'],DT_After_CM$byClass['F1'],
                                       RF_After_CM$byClass['F1']),
                        Recall_Score=c(DT_Before_CM$byClass['Recall'],DT_After_CM$byClass['Recall'],
                                       RF_After_CM$byClass['Recall']),
                        Accuracy_Score=c(DT_Before_CM$overall['Accuracy'],DT_After_CM$overall['Accuracy'],
                                     RF_After_CM$overall['Accuracy']))
models_table <- models_table %>% remove_rownames %>% column_to_rownames(var="models")

kable(models_table, "html") %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% column_spec(1, width = "8cm")

```

##Recommendation

* Duration has a positive effect on our outcome, the longer customer spends with a particular product, the more likely they are to be interested in using the product, so there are higher chances that they will opt for our services

* During the next campaign if we see customer spends more time, special attention and remarketing strategies must be implemented to make sure customer opts for the service

* Apart from that we can concentrate on the customers whose previous campaign outcome is success, they also have good chances of becoming our bank's customer
